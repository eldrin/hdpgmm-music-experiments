\documentclass{beamer}
%% Possible paper sizes: a0, a0b, a1, a2, a3, a4.
%% Possible orientations: portrait, landscape
%% Font sizes can be changed using the scale option.
\usepackage[size=a0,orientation=portrait,scale=1.2]{beamerposter}
\usetheme{LLT-poster}
% \usecolortheme{ComingClean}
\usecolortheme{Entrepreneur}
% \usecolortheme{ConspiciousCreep}  %% VERY garish.

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{libertine}
\usepackage[scaled=1]{inconsolata}
\usepackage[libertine]{newtxmath}
\usepackage[numbers]{natbib}
\usepackage{amsmath}
\usepackage[ruled]{algorithm2e}
\renewcommand{\bibfont}{\small}

\newcommand{\texthash}{\#}


%% Load the markdown package
\usepackage[citations,footnotes,definitionLists,hashEnumerators,smartEllipses,tightLists=false,pipeTables,tableCaptions,hybrid]{markdown}
%%begin novalidate
\markdownSetup{rendererPrototypes={
 link = {\href{#2}{#1}},
 headingFour = {\begin{block}{#1}},
 horizontalRule = {\end{block}}
}}
%%end novalidate

\author[j.h.kim@tudelft.nl]{Jaehun Kim and Cynthia C. S. Liem}
\title{The Power of Deep without Going Deep? A Study of HDPGMM Music Representation Learning}
\institute{Delft University of Technology}
% Optional foot image
\footimage{\includegraphics[width=12cm]{TUDelft_logo_black.png}}

\begin{document}
\begin{frame}[fragile]\centering

% \bigskip
% {\usebeamercolor[fg]{headline}\hrulefill}
% \bigskip


\begin{columns}[T]

%%%% First Column
\begin{column}{.46\textwidth}

\begin{markdown}


#### Motivation

* In late 2000s - early 2010s, MIR community explored Bayesian Nonparametric (BN) models.
* After Deep Learning (DL), there are few works exploring BNs.
* BN can offer advantages what DL provides, while better in interpretability.

----

#### Deep Learning vs. Bayesian Nonparametric

- **High learning capacity**:
    * *Universal approximation theorem vs. Nonparametric nature*
- **Robust to overfitting**:
    * *Dropout/Weight Decay/Augmentation/etc. vs. Bayesian nature*
- **Efficient learning algorithm**:
    * *SGD, ADAM, etc. vs. Online variational inference*
- **Can go "deep"**:
    * *Stacked layers vs. (nested) Hierarchical Dirichlet process prior*
- **Interpretability**:
    * *(almost) black box vs. can be much better*

----

#### Contributions

* Insight into how "good" and transferable the HDPGMM representation is for MIR tasks.
* An implementation of a GPU-accelerated inference algorithm for HDPGMM. [@package]

----

\bigskip
{\usebeamercolor[fg]{headline}\hrulefill}
\bigskip

#### Hierarchical DP Gaussian Mixture Model (HDPGMM)

* DP can draw distributions of an arbitrary dimensionality.
* One of the useful analogy to understand DP is "stick-breaking" process:

\begin{align}
    \beta^{\prime}\_{k} \sim \text{Beta}(1, \gamma) \qquad
    \beta\_{k} = \beta^{\prime}\_{k} \prod\_{l=1}^{k-1} (1 - \beta\_{l}^{\prime})
\end{align}


\setkeys{Gin}{width=.45\linewidth}
![stickbreaking](../ismir_submission/figs/stick-breaking.pdf "Illustration of stick-breaking construction")

* When $\beta$ is drawn in this way, we can refer it as $\beta \sim \text{GEM}(\gamma)$

* Employing DP prior as `mixing distribution`, DPMM can find appropriate number of components for given dataset.
* It is formally defined as follows:

\begin{equation}
\begin{aligned}[c]
    \beta|\gamma &\sim \text{GEM}(\gamma) \\\
    y\_{i}|\beta &\sim \text{Mult}(\beta) \\\
\end{aligned}
\qquad
\begin{aligned}[c]
    \phi\_{k}|H &\sim H \\\
    x\_{i}|y\_{i},\{\phi\_{k}\} &\sim F(\phi\_{y\_{i}}) \\\
\end{aligned}
\end{equation}


* DPMM can be extended to the 2-level hierarchy, learning global and group-level components.
* Group naturally arises in many domains, including MIR problems (i.e., lyrics-words, artist-songs, song-time instance features)
* In this work we set "corpus-level" time instance features as upper level, and "song" as a groups.

\setkeys{Gin}{width=.7\linewidth}
![hdpstickbreaking](../ismir_submission/figs/HDP-stick-breaking.pdf "Illustration of HDP stick-breaking construction")

* Song-level components "inherits" the global components with song-specific mixing coefficients $\pi\_{j}$.
* Setting $F$ as Gaussian-Inverse Wishart distribution and its parameters $\theta$ accordingly, we can model song features 

\begin{equation}
\begin{aligned}[c]
    \pi\_{j}|\alpha\_{0} &\sim \text{GEM}(\alpha\_{0}) \\\
    z\_{jn}|\pi\_{j} &\sim \text{Mult}(\pi\_{j})
\end{aligned}
\qquad
\begin{aligned}[c]
    \theta\_{jn} = \psi\_{jz\_{jn}} &= \phi\_{c\_{jz\_{jn}}}  \\\
    x\_{jn}|z\_{jn}, c\_{jt}, \{\phi\_{k}\} &\sim F(\theta\_{jn}) 
\end{aligned}
\end{equation}

---- 

\end{markdown}

\end{column}

%%%% Second Column
\begin{column}{.46\textwidth}

\begin{markdown}


#### Inference (Training) / Regularization / Representation

* **Online Variational Inference (OVI)** with the mean field (fully-factorized) approximation.
* Additionally we "splash" the uniform noise $e$ to the inferred responsibility $r\_{jn}$ per each time instance to take account the missing data due to the preview clipping.

\begin{align}
\tilde{r}\_{jn} = (1 - \eta\_{t}) r\_{jn} + \eta\_{t} e
\end{align}  

* We employ the (variational) expectation of log-likelihood of samples $\tilde{y}\_{jk} = \text{exp}(\mathbb{E}\_{q}\[\text{log}\,p(X\_{j}|c\_{j}, z\_{j}, \phi\_{k})\])$ as the song-level representation.

----

\bigskip
{\usebeamercolor[fg]{headline}\hrulefill}
\bigskip

#### Experimental Design

| Right | Left | Default | Center |
|------:|:-----|---------|:------:| 
|  12   |  12  |  12     |   12   | 
| 123   |  123 |   123   |  123   | 
|   1   |    1 |     1   |    1   | 

  : Demonstration of pipe table syntax.

----

\end{markdown}
\end{column}
\end{columns}

\begin{markdown}

#### Bibliography

\bibliographystyle{unsrtnat}
\bibliography{refs}

----

\end{markdown}

\end{frame}


\end{document}
