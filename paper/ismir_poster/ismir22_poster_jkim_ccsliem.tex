\documentclass{beamer}
%% Possible paper sizes: a0, a0b, a1, a2, a3, a4.
%% Possible orientations: portrait, landscape
%% Font sizes can be changed using the scale option.
\usepackage[size=a0,orientation=portrait,scale=1]{beamerposter}
\usetheme{LLT-poster}
% \usecolortheme{ComingClean}
\usecolortheme{Entrepreneur}
% \usecolortheme{ConspiciousCreep}  %% VERY garish.

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{libertine}
\usepackage[scaled=1.5]{inconsolata}
\usepackage[libertine]{newtxmath}
\usepackage[numbers]{natbib}
\usepackage{amsmath}
\usepackage[ruled]{algorithm2e}
\renewcommand{\bibfont}{\small}

\newcommand{\texthash}{\#}


%% Load the markdown package
\usepackage[citations,footnotes,definitionLists,hashEnumerators,smartEllipses,tightLists=false,pipeTables,tableCaptions,hybrid]{markdown}
%%begin novalidate
\markdownSetup{rendererPrototypes={
 link = {\href{#2}{#1}},
 headingFour = {\begin{block}{#1}},
 horizontalRule = {\end{block}}
}}
%%end novalidate

\author[j.h.kim@tudelft.nl]{Jaehun Kim and Cynthia C. S. Liem}
\title{\textbf{The Power of Deep without Going Deep?}\\ \textit{A Study of HDPGMM Music Representation Learning}}
\institute{Delft University of Technology}
% Optional foot image
\footimage{\includegraphics[width=6cm]{TUDelft_logo_black.png}}

\begin{document}
\begin{frame}[fragile]\centering

% \bigskip
% {\usebeamercolor[fg]{headline}\hrulefill}
% \bigskip


\begin{columns}[T]

%%%% First Column
\begin{column}{.46\textwidth}

\begin{markdown}

#### tl;dr

* Bayesian nonparametric models can learn music representations as effectively as Deep Learning while being more interpretable.


----


#### Motivation

* In the late 2000s - early 2010s, the MIR community explored Bayesian Nonparametric (BN) models.
* After Deep Learning (DL), there are few works exploring BNs.
* BN can offer advantages that DL provides while being more interpretable.

----

#### Deep Learning vs. Bayesian Nonparametric

- **High learning capacity**:
    * *Universal approximation theorem vs. Nonparametric nature*
- **Robust to overfitting**:
    * *Dropout/Weight Decay/Augmentation/etc. vs. Bayesian nature*
- **Efficient learning algorithm**:
    * *SGD, ADAM, etc. vs. Online variational inference*
- **Can go "deep"**:
    * *Stacked layers vs. (nested) Hierarchical Dirichlet process prior*
- **Interpretability**:
    * *(almost) black-box vs. can be much better*

----

#### Contributions

* Insight into how "good" and transferable the HDPGMM representation is for MIR tasks.
* An implementation of a GPU-accelerated inference algorithm for HDPGMM. [@package]

----

\bigskip
{\usebeamercolor[fg]{headline}\hrulefill}
\bigskip

#### Hierarchical Dirichlet Process Gaussian Mixture Model (HDPGMM)

* Dirichlet Process (DP) can draw distributions of arbitrary dimensionality.
* One of the useful analogies to understand DP is the "stick-breaking" process:

\begin{align}
    \beta^{\prime}\_{k} \sim \text{Beta}(1, \gamma) \qquad
    \beta\_{k} = \beta^{\prime}\_{k} \prod\_{l=1}^{k-1} (1 - \beta\_{l}^{\prime})
\end{align}


\setkeys{Gin}{width=.45\linewidth}
![stickbreaking](../ismir_submission/figs/stick-breaking.pdf "Illustration of stick-breaking construction")

* When $\beta$ is drawn in this way, we can refer it as $\beta \sim \text{GEM}(\gamma)$

* Employing DP prior as *mixing distribution*, DPMM can find an appropriate number of components for a given dataset.
* It is formally defined as follows:

\begin{equation}
\begin{aligned}[c]
    \beta|\gamma &\sim \text{GEM}(\gamma) \\\
    y\_{i}|\beta &\sim \text{Mult}(\beta) \\\
\end{aligned}
\qquad
\begin{aligned}[c]
    \phi\_{k}|H &\sim H \\\
    x\_{i}|y\_{i},\{\phi\_{k}\} &\sim F(\phi\_{y\_{i}}) \\\
\end{aligned}
\end{equation}


* DPMM can be extended to the 2-level hierarchy, learning global and group-level components.
* Group naturally arises in many domains, including MIR problems (i.e., lyrics-words, artist-songs, song-time instance features)
* In this work, we set "corpus-level" time instance features as the upper level and "song" as a group of features, being the lower level.

\setkeys{Gin}{width=.6\linewidth}
![hdpstickbreaking](../ismir_submission/figs/HDP-stick-breaking.pdf "Illustration of HDP stick-breaking construction")

* Song-level components "inherits" the global components with song-specific mixing coefficients $\pi\_{j}$.
* Setting $F$ as Gaussian-Inverse Wishart distribution and its parameters $\theta$ accordingly, we can model song features 

\begin{equation}
\begin{aligned}[c]
    \pi\_{j}|\alpha\_{0} &\sim \text{GEM}(\alpha\_{0}) \\\
    z\_{jn}|\pi\_{j} &\sim \text{Mult}(\pi\_{j})
\end{aligned}
\qquad
\begin{aligned}[c]
    \theta\_{jn} = \psi\_{jz\_{jn}} &= \phi\_{c\_{jz\_{jn}}}  \\\
    x\_{jn}|z\_{jn}, c\_{jt}, \{\phi\_{k}\} &\sim F(\theta\_{jn}) 
\end{aligned}
\end{equation}

---- 


#### Inference (Training) / Regularization / Representation / Input Features

* **Online Variational Inference (OVI)** with the mean-field (fully-factorized) approximation.
* Additionally, we "splash" the uniform noise $e$ to the inferred responsibility $r\_{jn}$ each time instance to account for the missing data due to the preview clipping.

\begin{align}
\tilde{r}\_{jn} = (1 - \eta\_{t}) r\_{jn} + \eta\_{t} e
\end{align}  

* We employ the (variational) expectation of log-likelihood of samples $\tilde{y}\_{jk} = \text{exp}(\mathbb{E}\_{q}\[\text{log}\,p(X\_{j}|c\_{j}, z\_{j}, \phi\_{k})\])$ as the song-level representation.
* Following [@DBLP:journals/taffco/WangLCCH15], we employ a set of music audio features as the input features for HDPGMM models.
    - *52 Dimensions*: MFCC (13), $\Delta$MFCC (13), $\Delta\Delta$MFCC (13), Onset Strength (1), Chroma (2)

----


\end{markdown}
\end{column}

%%%% Second Column
\begin{column}{.46\textwidth}

\begin{markdown}


% #### Input Features
% 
% * Following [@DBLP:journals/taffco/WangLCCH15], we employ a set of music audio features as the input features for HDPGMM models.
% 
% | Feature | Aspect | Dim. | Transform |
% |:-------:|:------:|:----:|:---------:|
% | MFCC    | Timbre | 13   | -         |
% | $\Delta$MFCC | Timbre | 13 | -      |
% | $\Delta\Delta$MFCC | Timbre | 13 | - |
% | Onset Strength | Rhythm | 1 | $\text{log}$ |
% | Chroma | Tonal | 12 | $\text{log}$ |
% 
%   : Audio features employed for HDPGMM 
% 
% ----

\bigskip
{\usebeamercolor[fg]{headline}\hrulefill}
\bigskip

#### Experimental Design

* several models compared
    - **G1**: single multivariate Gaussian parameters (mean-sd) per song
    - **VQCodebook**: approximation of HDPGMM, fitting K-Means globally and employing the post-hoc component frequency per song as the representation.
    - **KIM**: VGG-ish convolutional neural network taking stereo mel-spectrogram as input feature, which is trained with a simple self-supervision objective.
    - **CLMR**: recent DL-based music representations employing advanced self-supervision objective (contrastive learning). It takes time-domain audio samples as input.


* three commonly used MIR downstream tasks are considered:

| Dataset | Purpose | no. Samples  | no. Classes/no. Users  | Acc. Measure |
|:-------:|:-------:|:------------:|:----------------------:|:------------:|
|MSD      | Repr. Learning | $213,354$ | N/A               | N/A          |
|Echonest | Recommendation | $40,980$  | $571,355$         | nDCG         |
|GTZAN    | Genre Clf. | $1,000$   | $10$              | F1           |
|MTAT     | Autotagging | $25,863$  | $50$              | AUROC        |

  : Dataset for training representation (MSD) and downstream tasks evaluation (rest)

----

#### Main Results

\setkeys{Gin}{width=.75\linewidth}
![mainresult](../ismir_submission/figs/main_result_plot.pdf "Main downstream task evaluation results.")

* HDPGMM shows the overall comparable "performance" against DL-based representations within our experimental setup.
* HDPGMM representations are competitive to DLs on GTZAN and MTAT, while DL models outperform HDPGMM on Echonest.
* Overall, HDPGMM outperforms simpler non-DL baselines, except on Echonest.

----

#### Hyper Parameter Tuning for HDPGMM

\setkeys{Gin}{width=.55\linewidth}
![regularizationeffect](../ismir_submission/figs/regularization_effect.pdf "Effect of regularization factor.")

* The additional regularization shows an apparent positive effect up to the range we tested.
* It suggests that employing full-length songs would possibly improve the representation further.


![datasetsizeeffect](../ismir_submission/figs/num_sample_effect.pdf "Effect of the number of training samples.")

* The number of training samples also generally indicates a positive effect on the quality of the representation.
* However, it is logarithmic than linear, which suggests:
    - HDPGMM model already generalizes well on the smaller dataset, or
    - It requires exponentially more data to become more competent.

----


#### Interpretability

* Knowing what each part of the probabilistic model is supposed to mean and estimating the meaning of components give us a good sense of interpretable representation.
* By intermediating the song-tag assignment matrix from MSD, the semantics of components can be estimated.


|  Comp1         |  Comp2            |  Comp3            |  Comp4           | Comp5       |
|:--------------:|:-----------------:|:-----------------:|:----------------:|:-----------:|
| Hip-Hop        | country           | female vocalists  | pop              | electronic  |
| pop            | rock              | singer-songwriter | female vocalists | dance       |
| rnb            | pop               | pop               | female vocalist  | electronica |
| soul           | oldies            | acoustic          | rock             | funk        |
| male vocalists | indie             | Mellow            | Love             | electro     |

  : Example of tag-based estimation of the per-component semantics.

----

#### Conclusion \& Future Works

- BN models can learn music representation as effectively as DL while being more interpretable.
- There are several ways to extend BN models
    * semi-supervised learning
    * "deeper" latent structure (nested HDP)
    * sequence-aware models (infinite HMM)

----


\end{markdown}
\end{column}
\end{columns}

\begin{markdown}

#### Bibliography

\bibliographystyle{unsrtnat}
\bibliography{refs}

----

\end{markdown}

\end{frame}


\end{document}
